{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model for Kaggle chaii competition.\n",
    "## In this competition we are suppose to predict Answer (index) given the question and context document\n",
    "## Data Language is Hindi and Tamil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.kaggle.com/jirkaborovec/chaii-q-a-with-pytorch-lightining\n",
    "- https://www.kaggle.com/theamitnikhade/question-answering-starter-roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T20:25:08.670705Z",
     "iopub.status.busy": "2021-10-05T20:25:08.670341Z",
     "iopub.status.idle": "2021-10-05T20:25:12.512270Z",
     "shell.execute_reply": "2021-10-05T20:25:12.511255Z",
     "shell.execute_reply.started": "2021-10-05T20:25:08.670610Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer,AutoModelForQuestionAnswering,DataCollatorWithPadding\n",
    "from transformers import get_scheduler\n",
    "from transformers import AdamW\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T20:25:12.515695Z",
     "iopub.status.busy": "2021-10-05T20:25:12.514807Z",
     "iopub.status.idle": "2021-10-05T20:25:12.553599Z",
     "shell.execute_reply": "2021-10-05T20:25:12.552643Z",
     "shell.execute_reply.started": "2021-10-05T20:25:12.515649Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T20:25:12.555750Z",
     "iopub.status.busy": "2021-10-05T20:25:12.555401Z",
     "iopub.status.idle": "2021-10-05T20:25:12.565308Z",
     "shell.execute_reply": "2021-10-05T20:25:12.564128Z",
     "shell.execute_reply.started": "2021-10-05T20:25:12.555703Z"
    }
   },
   "outputs": [],
   "source": [
    "## path for kaggel notebook\n",
    "data_dir = '/kaggle/input/chaii-hindi-and-tamil-question-answering/'\n",
    "output_dir = '/kaggle/output/kaggle/working/'\n",
    "\n",
    "train_fn = 'train.csv'\n",
    "test_fn='test.csv'\n",
    "sub_fn = 'sample_submission.csv'\n",
    "\n",
    "BATCH_SIZE= 8\n",
    "#MAX_LENGTH = 512\n",
    "MAX_SEQ_LENGTH = 384\n",
    "DOC_STRIDE = 128\n",
    "lr = 5e-5\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T20:25:12.567357Z",
     "iopub.status.busy": "2021-10-05T20:25:12.566999Z",
     "iopub.status.idle": "2021-10-05T20:25:12.960865Z",
     "shell.execute_reply": "2021-10-05T20:25:12.959797Z",
     "shell.execute_reply.started": "2021-10-05T20:25:12.567315Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(data_dir+train_fn)\n",
    "test_df = pd.read_csv(data_dir+test_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T20:25:12.965834Z",
     "iopub.status.busy": "2021-10-05T20:25:12.965613Z",
     "iopub.status.idle": "2021-10-05T20:25:12.971908Z",
     "shell.execute_reply": "2021-10-05T20:25:12.971026Z",
     "shell.execute_reply.started": "2021-10-05T20:25:12.965807Z"
    }
   },
   "outputs": [],
   "source": [
    "example_id2idx = { eid:idx for idx,eid in enumerate(train_df['id'].values)}\n",
    "example_idx2id = { idx:eid for idx,eid in enumerate(train_df['id'].values)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking if the given answer start (index) label is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T20:25:12.974467Z",
     "iopub.status.busy": "2021-10-05T20:25:12.973717Z",
     "iopub.status.idle": "2021-10-05T20:25:13.482374Z",
     "shell.execute_reply": "2021-10-05T20:25:13.481378Z",
     "shell.execute_reply.started": "2021-10-05T20:25:12.974427Z"
    }
   },
   "outputs": [],
   "source": [
    "corp_lens = []\n",
    "for idx in range(0,train_df.shape[0]):\n",
    "    subdf = train_df.iloc[idx]\n",
    "    strt = subdf['answer_start']\n",
    "    end = strt+len(subdf['answer_text'])\n",
    "    ans = subdf['context'][strt:end]\n",
    "    \n",
    "    corp_lens.append(len(subdf['context'].split()))\n",
    "    if not ans==subdf['answer_text']:\n",
    "        print(idx,ans,'****',subdf['answer_text'])\n",
    "        \n",
    "## This is very rough estimation of corpus length. It doesnt represent actual token lenght from tokenizer        \n",
    "print(f'top 10 token len of the corpus in training dataset: {sorted(corp_lens,reverse=True)[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T20:25:13.484781Z",
     "iopub.status.busy": "2021-10-05T20:25:13.484281Z",
     "iopub.status.idle": "2021-10-05T20:25:13.546810Z",
     "shell.execute_reply": "2021-10-05T20:25:13.545625Z",
     "shell.execute_reply.started": "2021-10-05T20:25:13.484741Z"
    }
   },
   "outputs": [],
   "source": [
    "## function to generate answer end using start index and answer\n",
    "def add_end_index(row):\n",
    "    strt = row['answer_start']\n",
    "    end = strt+len(row['answer_text'])\n",
    "    ans = row['context'][strt:end]\n",
    "    assert(ans==row['answer_text'])\n",
    "    return end\n",
    "train_df['answer_end'] = train_df.apply(add_end_index,axis=1)\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T20:25:13.549124Z",
     "iopub.status.busy": "2021-10-05T20:25:13.548766Z",
     "iopub.status.idle": "2021-10-05T20:25:13.559693Z",
     "shell.execute_reply": "2021-10-05T20:25:13.558451Z",
     "shell.execute_reply.started": "2021-10-05T20:25:13.549083Z"
    }
   },
   "outputs": [],
   "source": [
    "## random shuffle and splitting data set\n",
    "## ratio is 85-15\n",
    "train_df = train_df.sample(frac=1)\n",
    "\n",
    "split_idx = int(0.85*train_df.shape[0])\n",
    "valid_df = train_df.iloc[split_idx:].reset_index(drop=True)\n",
    "train_df = train_df.iloc[:split_idx].reset_index(drop=True)\n",
    "\n",
    "print(train_df.shape,valid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T20:25:13.562241Z",
     "iopub.status.busy": "2021-10-05T20:25:13.561613Z",
     "iopub.status.idle": "2021-10-05T20:25:13.697786Z",
     "shell.execute_reply": "2021-10-05T20:25:13.696791Z",
     "shell.execute_reply.started": "2021-10-05T20:25:13.562195Z"
    }
   },
   "outputs": [],
   "source": [
    "## creating Dataset from pandas. This is hugging face dataset not pytorch\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "valid_dataset = Dataset.from_pandas(valid_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the model and tokenizer using pretrain weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T20:25:13.701885Z",
     "iopub.status.busy": "2021-10-05T20:25:13.701651Z",
     "iopub.status.idle": "2021-10-05T20:25:23.940915Z",
     "shell.execute_reply": "2021-10-05T20:25:23.940064Z",
     "shell.execute_reply.started": "2021-10-05T20:25:13.701858Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrain_path = 'ai4bharat/indic-bert'\n",
    "# pretrain_path = '/kaggle/input/indicbert/indic-bert-v1'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrain_path)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(pretrain_path)\n",
    "model = model.to(device)\n",
    "## data collator to prepare batch data\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T20:25:23.949580Z",
     "iopub.status.busy": "2021-10-05T20:25:23.946798Z",
     "iopub.status.idle": "2021-10-05T20:25:24.210251Z",
     "shell.execute_reply": "2021-10-05T20:25:24.209316Z",
     "shell.execute_reply.started": "2021-10-05T20:25:23.949460Z"
    }
   },
   "outputs": [],
   "source": [
    "all_vocab = tokenizer.get_vocab()\n",
    "print(f'number of vocab our tokenizer have {len(all_vocab)}')\n",
    "print(f'printing ... few **vocab** {list(all_vocab.keys())[:10]} and their **index** {list(all_vocab.values())[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T20:25:24.218111Z",
     "iopub.status.busy": "2021-10-05T20:25:24.215450Z",
     "iopub.status.idle": "2021-10-05T20:25:24.242198Z",
     "shell.execute_reply": "2021-10-05T20:25:24.241222Z",
     "shell.execute_reply.started": "2021-10-05T20:25:24.218066Z"
    }
   },
   "outputs": [],
   "source": [
    "## function to create token out of each question and context pair.\n",
    "## context and question pair are concat together\n",
    "def dataprep(data):\n",
    "    \n",
    "    data['context'] = [context.strip() for context in data['context']]\n",
    "    data['question'] = [question.strip() for question in data['question']]\n",
    "    \n",
    "    tokenizer_output = tokenizer(data['question'],\n",
    "                                 data['context'],\n",
    "                                truncation=\"only_second\",\n",
    "                                max_length=MAX_SEQ_LENGTH,\n",
    "                                stride=DOC_STRIDE,\n",
    "                                return_overflowing_tokens=True,\n",
    "                                return_offsets_mapping=True,\n",
    "                                padding=\"max_length\",)\n",
    "    \n",
    "    \n",
    "    sample_mapping = tokenizer_output.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenizer_output.pop(\"offset_mapping\")\n",
    "    \n",
    "    tokenizer_output[\"start_positions\"] = []\n",
    "    tokenizer_output[\"end_positions\"] = []\n",
    "        \n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        feature = {}\n",
    "        input_ids = tokenizer_output[\"input_ids\"][i]\n",
    "        attention_mask = tokenizer_output[\"attention_mask\"][i]\n",
    "        \n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        sequence_ids = tokenizer_output.sequence_ids(i)\n",
    "        \n",
    "        sample_index = sample_mapping[i]\n",
    "        answer_text = data['answer_text'][sample_index]\n",
    "        answer_start = data['answer_start'][sample_index]\n",
    "        answer_end = data['answer_end'][sample_index]\n",
    "\n",
    "        if len(str(answer_start)) == None:\n",
    "            tokenizer_output[\"start_positions\"].append(cls_index)\n",
    "            tokenizer_output[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "\n",
    "                \n",
    "            # 01111111110\n",
    "            # [CLS]ccccccc[SEP]\n",
    "                \n",
    "            if not (offsets[token_start_index][0] <= answer_start and offsets[token_end_index][1] >= answer_end):\n",
    "                tokenizer_output[\"start_positions\"].append(cls_index)\n",
    "                tokenizer_output[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= answer_start:\n",
    "                    token_start_index += 1\n",
    "                tokenizer_output[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= answer_end:\n",
    "                    token_end_index -= 1\n",
    "                tokenizer_output[\"end_positions\"].append(token_end_index + 1)\n",
    "        \n",
    "    return tokenizer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T20:25:24.250580Z",
     "iopub.status.busy": "2021-10-05T20:25:24.247563Z",
     "iopub.status.idle": "2021-10-05T20:26:05.143745Z",
     "shell.execute_reply": "2021-10-05T20:26:05.142645Z",
     "shell.execute_reply.started": "2021-10-05T20:25:24.250535Z"
    }
   },
   "outputs": [],
   "source": [
    "## creating token using dataprep function\n",
    "tokenized_train_ds = train_dataset.map(dataprep,batched=True,remove_columns=train_dataset.column_names)\n",
    "tokenized_train_ds.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T20:26:05.148904Z",
     "iopub.status.busy": "2021-10-05T20:26:05.148669Z",
     "iopub.status.idle": "2021-10-05T20:26:05.160427Z",
     "shell.execute_reply": "2021-10-05T20:26:05.159330Z",
     "shell.execute_reply.started": "2021-10-05T20:26:05.148876Z"
    }
   },
   "outputs": [],
   "source": [
    "## function to create token out of each question and context pair.\n",
    "## context and question pair are concat together\n",
    "def dataprep_validation(data):\n",
    "    \n",
    "    data['context'] = [context.strip() for context in data['context']]\n",
    "    data['question'] = [question.strip() for question in data['question']]\n",
    "    \n",
    "    tokenizer_output = tokenizer(data['question'],\n",
    "                                 data['context'],\n",
    "                                truncation=\"only_second\",\n",
    "                                max_length=MAX_SEQ_LENGTH,\n",
    "                                stride=DOC_STRIDE,\n",
    "                                return_overflowing_tokens=True,\n",
    "                                return_offsets_mapping=True,\n",
    "                                padding=\"max_length\",)\n",
    "    \n",
    "    \n",
    "    sample_mapping = tokenizer_output.pop(\"overflow_to_sample_mapping\")\n",
    "    tokenizer_output['example_id'] = []\n",
    "    \n",
    "    for i in range(len(tokenizer_output[\"input_ids\"])):\n",
    "        sequence_ids = tokenizer_output.sequence_ids(i)\n",
    "        sample_index = sample_mapping[i]\n",
    "        \n",
    "        #print(sample_index)\n",
    "        tokenizer_output[\"example_id\"].append(example_id2idx[data[\"id\"][sample_index]])\n",
    "        \n",
    "        updated_offset_list = []\n",
    "        old_offset_list = tokenizer_output[\"offset_mapping\"][i]\n",
    "        for k, o in enumerate(old_offset_list):\n",
    "            if sequence_ids[k] == 1:\n",
    "                updated_offset_list.append(o)\n",
    "            else:\n",
    "                updated_offset_list.append((-1,-1))\n",
    "                \n",
    "        tokenizer_output[\"offset_mapping\"][i] = updated_offset_list\n",
    "        \n",
    "    return tokenizer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T20:26:05.163357Z",
     "iopub.status.busy": "2021-10-05T20:26:05.162543Z",
     "iopub.status.idle": "2021-10-05T20:26:21.896456Z",
     "shell.execute_reply": "2021-10-05T20:26:21.895312Z",
     "shell.execute_reply.started": "2021-10-05T20:26:05.163302Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_valid_ds = valid_dataset.map(dataprep_validation,batched=True,remove_columns=valid_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T20:26:21.898620Z",
     "iopub.status.busy": "2021-10-05T20:26:21.898226Z",
     "iopub.status.idle": "2021-10-05T20:26:29.268891Z",
     "shell.execute_reply": "2021-10-05T20:26:29.267794Z",
     "shell.execute_reply.started": "2021-10-05T20:26:21.898576Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_valid_ds_updated = tokenized_valid_ds.map(lambda data: data, remove_columns=['example_id', 'offset_mapping'])\n",
    "tokenized_valid_ds_updated.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T20:26:29.271502Z",
     "iopub.status.busy": "2021-10-05T20:26:29.270461Z",
     "iopub.status.idle": "2021-10-05T20:26:29.352343Z",
     "shell.execute_reply": "2021-10-05T20:26:29.351242Z",
     "shell.execute_reply.started": "2021-10-05T20:26:29.271458Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_train_ds.data.to_pandas().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T20:26:29.354369Z",
     "iopub.status.busy": "2021-10-05T20:26:29.353903Z",
     "iopub.status.idle": "2021-10-05T20:26:30.273765Z",
     "shell.execute_reply": "2021-10-05T20:26:30.267850Z",
     "shell.execute_reply.started": "2021-10-05T20:26:29.354323Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_df = tokenized_valid_ds.data.to_pandas()\n",
    "eval_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T20:26:30.276909Z",
     "iopub.status.busy": "2021-10-05T20:26:30.276169Z",
     "iopub.status.idle": "2021-10-05T20:26:30.285452Z",
     "shell.execute_reply": "2021-10-05T20:26:30.283327Z",
     "shell.execute_reply.started": "2021-10-05T20:26:30.276864Z"
    }
   },
   "outputs": [],
   "source": [
    "## Using pytorch DataLoader function \n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_train_ds, shuffle=True, batch_size=BATCH_SIZE, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_valid_ds_updated, batch_size=BATCH_SIZE, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T20:26:30.288677Z",
     "iopub.status.busy": "2021-10-05T20:26:30.287595Z",
     "iopub.status.idle": "2021-10-05T20:26:30.303187Z",
     "shell.execute_reply": "2021-10-05T20:26:30.302131Z",
     "shell.execute_reply.started": "2021-10-05T20:26:30.288630Z"
    }
   },
   "outputs": [],
   "source": [
    "## learning rate decay scheduler\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T20:26:30.311644Z",
     "iopub.status.busy": "2021-10-05T20:26:30.308638Z",
     "iopub.status.idle": "2021-10-05T20:26:30.355065Z",
     "shell.execute_reply": "2021-10-05T20:26:30.354133Z",
     "shell.execute_reply.started": "2021-10-05T20:26:30.311588Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "def jacard_score_(data_dict):\n",
    "    score = 0\n",
    "    for data in data_dict:\n",
    "        p,tp = list(data.values())[0]\n",
    "        if p is not '' and tp is not '':\n",
    "            score += jaccard(p,tp)\n",
    "    return score\n",
    "\n",
    "def transform_logits(logits):\n",
    "    \n",
    "    new_logits = np.array(logits[0])\n",
    "    for logit in logits[1:]:\n",
    "        new_logits = np.vstack((new_logits, logit))\n",
    "    \n",
    "    return new_logits\n",
    "\n",
    "## extracting answer from logits\n",
    "def postprocess(val_df,eval_df,all_start_logits,all_end_logits,idx2id,n_best_size=20,max_answer_length=30):\n",
    "    \n",
    "    all_start_logits = transform_logits(all_start_logits)\n",
    "    all_end_logits = transform_logits(all_end_logits)\n",
    "    \n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    all_features = []\n",
    "    for i, feature in enumerate(eval_df):\n",
    "        features_per_example[idx2id[feature['example_id']]].append(i)\n",
    "        all_features.append(i)\n",
    "            \n",
    "    predictions = collections.OrderedDict()\n",
    "    \n",
    "    for idx, data  in enumerate(val_df):\n",
    "        feature_indices = features_per_example[data[\"id\"]]\n",
    "        valid_ans = []\n",
    "        \n",
    "        context = data[\"context\"]\n",
    "        for feature_index in feature_indices:\n",
    "            \n",
    "            offset_mapping = eval_df[feature_index][\"offset_mapping\"]\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index][0] == -1\n",
    "                        or offset_mapping[end_index][0] == -1\n",
    "                    ):\n",
    "                        continue\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_ans.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "                    \n",
    "        if len(valid_ans) > 0:\n",
    "            best_answer = sorted(valid_ans, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        predictions[data[\"id\"]] = best_answer[\"text\"]\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "## evaluation \n",
    "def model_eval(model,val_df,evaldataset_,eval_df,idx2id):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    n = len(evaldataset_)\n",
    "    \n",
    "    all_start_logits = []\n",
    "    all_end_logits = []\n",
    "    \n",
    "    for batch in evaldataset_:\n",
    "        batch_ = {k: v.to(device) for k, v in batch.items() if k != 'example_id'}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch_)\n",
    "        \n",
    "        slogits = outputs.start_logits.cpu().detach().numpy()\n",
    "        elogits = outputs.end_logits.cpu().detach().numpy()\n",
    "        \n",
    "        all_start_logits.append(slogits)\n",
    "        all_end_logits.append(elogits)\n",
    "\n",
    "    preds = postprocess(val_df,eval_df,all_start_logits,all_end_logits,idx2id)\n",
    "    jac_score = jacard_score_(preds)\n",
    "    print(f'jaccard score is {jac_score/n}')\n",
    "\n",
    "## training\n",
    "def train(model,traindata,val_df,eval_dataloader,eval_df,progress_steps,epochs):\n",
    "    progress_bar = tqdm(range(progress_steps))\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in traindata:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            batch_ = {k: v.to(device) for k, v in batch.items() if k != 'example_id'}\n",
    "            outputs = model(**batch_)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.update(1)\n",
    "            \n",
    "        print(f'training loss for epoch {epoch} is {epoch_loss/len(traindata)}')\n",
    "        model_eval(model,val_df,eval_dataloader,eval_df,example_idx2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T20:26:30.364330Z",
     "iopub.status.busy": "2021-10-05T20:26:30.361330Z"
    }
   },
   "outputs": [],
   "source": [
    "train(model,train_dataloader,valid_dataset,eval_dataloader,tokenized_valid_ds,num_training_steps,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texample_id2idx = { eid:idx for idx,eid in enumerate(test_df['id'].values)}\n",
    "texample_idx2id = { idx:eid for idx,eid in enumerate(test_df['id'].values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataprep_test(data):\n",
    "    \n",
    "    data['context'] = [context.strip() for context in data['context']]\n",
    "    data['question'] = [question.strip() for question in data['question']]\n",
    "    \n",
    "    tokenizer_output = tokenizer(data['question'],\n",
    "                                 data['context'],\n",
    "                                truncation=\"only_second\",\n",
    "                                max_length=MAX_SEQ_LENGTH,\n",
    "                                stride=DOC_STRIDE,\n",
    "                                return_overflowing_tokens=True,\n",
    "                                return_offsets_mapping=True,\n",
    "                                padding=\"max_length\",)\n",
    "    \n",
    "    \n",
    "    sample_mapping = tokenizer_output.pop(\"overflow_to_sample_mapping\")\n",
    "    tokenizer_output['example_id'] = []\n",
    "    \n",
    "    for i in range(len(tokenizer_output[\"input_ids\"])):\n",
    "        sequence_ids = tokenizer_output.sequence_ids(i)\n",
    "        sample_index = sample_mapping[i]\n",
    "        \n",
    "        tokenizer_output[\"example_id\"].append(texample_id2idx[data[\"id\"][sample_index]])\n",
    "        \n",
    "        updated_offset_list = []\n",
    "        old_offset_list = tokenizer_output[\"offset_mapping\"][i]\n",
    "        for k, o in enumerate(old_offset_list):\n",
    "            if sequence_ids[k] == 1:\n",
    "                updated_offset_list.append(o)\n",
    "            else:\n",
    "                updated_offset_list.append((-1,-1))\n",
    "                \n",
    "        tokenizer_output[\"offset_mapping\"][i] = updated_offset_list\n",
    "    \n",
    "    return tokenizer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_ds = test_dataset.map(dataprep_test,batched=True,remove_columns=test_dataset.column_names)\n",
    "tokenized_test_updated = tokenized_test_ds.map(lambda data: data, remove_columns=['example_id', 'offset_mapping'])\n",
    "tokenized_test_updated.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval_df = tokenized_test_ds.data.to_pandas()\n",
    "test_eval_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(\n",
    "    tokenized_test_updated, batch_size=BATCH_SIZE, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(predset,idx2id, test_dataset, test_eval_df):\n",
    "    \n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    all_start_logits = []\n",
    "    all_end_logits = []\n",
    "    \n",
    "    for batch in predset:\n",
    "        batch_ = {k: v.to(device) for k, v in batch.items() if k != 'example_id'}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch_)\n",
    "        \n",
    "        slogits = outputs.start_logits.cpu().detach().numpy()\n",
    "        elogits = outputs.end_logits.cpu().detach().numpy()\n",
    "        \n",
    "        all_start_logits.append(slogits)\n",
    "        all_end_logits.append(elogits)\n",
    "\n",
    "    preds = postprocess(test_dataset,test_eval_df,all_start_logits,all_end_logits,idx2id)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(test_dataloader,texample_idx2id, test_dataset, tokenized_test_ds)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in predictions.keys():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_sub(idx):\n",
    "    return predictions[idx]\n",
    "\n",
    "test_df['PredictionString'] = test_df.id.apply(prep_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.PredictionString.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.drop(columns=['context','question','language'], axis=1)\n",
    "test_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
